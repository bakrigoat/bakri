//////facebook
import os
os.getcwd()
import pandas as pd
df=pd.read_csv(r"/content/dataset_Facebook.csv",sep=';')
df
#describing the dataframe
df.describe()
df.shape
#1.) creating 3 subsets
df1=df[['Page total likes','Category','Post Month','Post Weekday']].loc[0:15]
df1
df2=df[['Page total likes','Category','Post Month','Post Weekday']].loc[16:30]
df2
df3=df[['Page total likes','Category','Post Month','Post Weekday']].loc[31:50]
df3
#merging (concatinating) subset1,2,3)
merging=pd.concat([df1,df2,df3])
merging
#sort data
sort_values=df.sort_values('Page total likes',ascending=False )
sort_values
#transposing data= columns converted into rows
df.transpose()
#shape of data
shaping=df.shape
shaping
#reshape data using pivot table method
pivot_table=pd.pivot_table(df,index=['Type','Category'],values='comment')
pivot_table
import numpy as np
reshaping_arr =np.array([1, 2, 3, 4, 5, 6])
reshaping_arr.reshape(3,2)



***********************************************************************************************************************************************


//////poco mobile
import requests
import bs4
#requests is used to request access
#bs4 is used to pull data
#getting data requests
request1= requests.get(' https://bit.ly/3MpHH3u')
request1
request1.content
soup= bs4.BeautifulSoup(request1.text)
soup
#fetching reviews and comments
reviews= soup.find_all('div',{'class':'t-ZKy'});
for review in reviews:
  print(review.get_text() +'\n\n')
reviews
#fetch the rating
#overall class ratings
#ratings = soup.find('div',{'class':'_2e3Uck'}).get_text();
#print(ratings)
# Check if the element with class '_2e3Uck' exists before calling get_text()
ratings_element = soup.find('div', {'class': '_2e3Uck'})
if ratings_element:
    ratings = ratings_element.get_text()
    print(ratings)
else:
    print("Element with class '_2e3Uck' not found")
#individual ratings
in_rating= soup.findAll('div',{'class':'_2LE14f'});
for indi_rating in in_rating:
  print(indi_rating.get_text()+ '\n')
#fetching tags
#tags = soup.find('span',{'class':'yhB1nd GXgmTe'}).get_text();
#tags
# Check if the element with class 'yhB1nd GXgmTe' exists before calling get_text()
tags_element = soup.find('span', {'class': 'yhB1nd GXgmTe'})
if tags_element:
    tags = tags_element.get_text()
    print(tags)
else:
    print("Element with class 'yhB1nd GXgmTe' not found")
#fetching customer name
#customer_name = soup.findAll('p',{"class"="_2sc7ZR _2V5EHH"});
#for customer in customer_name:
  #print(customer.get_text() + '\n')

# Find all <p> elements with class '_2sc7ZR _2V5EHH'
customer_names = soup.findAll('p', {'class': '_2sc7ZR _2V5EHH'})

# Check if any elements are found
if customer_names:
    # Iterate over each found element and print its text content
    for customer in customer_names:
        print(customer.get_text() + '\n')
else:
    print("No customer names found")




*******************************************************************************************************************************




import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
import os
#print(os.listdir("../Downloads"))
# Any results you write to the current directory are saved as output.
# We are reading our data
df = pd.read_csv("/content/heart.csv")
# First 5 rows of our data
df.head(10)
df.target.value_counts()
sns.countplot(x="target", data=df, palette="bwr")
plt.show()
sns.countplot(x="target", data=df, palette="bwr")

countNoDisease = len(df[df.target == 0])
countHaveDisease = len(df[df.target == 1])
print("Percentage of Patients Haven't Heart Disease: {:.2f}%".format((countNoDisease / (len(df.target))*100)))
print("Percentage of Patients Have Heart Disease: {:.2f}%".format((countHaveDisease / (len(df.target))*100)))
Percentage of Patients Haven't Heart Disease: 45.54%
Percentage of Patients Have Heart Disease: 54.46%

sns.countplot(x='sex', data=df, palette="mako_r")
plt.xlabel("Sex (0 = female, 1= male)")
plt.show()
sns.countplot(x='sex', data=df, palette="mako_r")

countFemale = len(df[df.sex == 0])
countMale = len(df[df.sex == 1])

print("Percentage of Female Patients: {:.2f}%".format((countFemale / (len(df.sex))*100)))
print("Percentage of Male Patients: {:.2f}%".format((countMale / (len(df.sex))*100)))
Percentage of Female Patients: 31.68%
Percentage of Male Patients: 68.32%

df.groupby('target').mean()

pd.crosstab(df.age,df.target).plot(kind="bar",figsize=(20,6))
plt.title('Heart Disease Frequency for Ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.savefig('heartDiseaseAndAges.png')
plt.show()

pd.crosstab(df.sex,df.target).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111' ])
plt.title('Heart Disease Frequency for Sex')
plt.xlabel('Sex (0 = Female, 1 = Male)')
plt.xticks(rotation=0)
plt.legend(["Haven't Disease", "Have Disease"])
plt.ylabel('Frequency')
plt.show()

plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c="red")
plt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])
plt.legend(["Disease", "Not Disease"])
plt.xlabel("Age")
plt.ylabel("Maximum Heart Rate")
plt.show()

pd.crosstab(df.slope,df.target).plot(kind="bar",figsize=(15,6),color=['#DAF7A6','#FF5733' ])
plt.title('Heart Disease Frequency for Slope')
plt.xlabel('The Slope of The Peak Exercise ST Segment ')
plt.xticks(rotation = 0)
plt.ylabel('Frequency')
plt.show()

pd.crosstab(df.fbs,df.target).plot(kind="bar",figsize=(15,6),color=['#FFC300','#581845' ])
plt.title('Heart Disease Frequency According To FBS')
plt.xlabel('FBS - (Fasting Blood Sugar > 120 mg/dl) (1 = true; 0 = false)')
plt.xticks(rotation = 0)
plt.legend(["Haven't Disease", "Have Disease"])
plt.ylabel('Frequency of Disease or Not')
plt.show()

pd.crosstab(df.cp,df.target).plot(kind="bar",figsize=(15,6),color=['#11A5AA','#AA1190' ])
plt.title('Heart Disease Frequency According To Chest Pain Type')
plt.xlabel('Chest Pain Type')
plt.xticks(rotation = 0)
plt.ylabel('Frequency of Disease or Not')
plt.show()

a = pd.get_dummies(df['cp'], prefix = "cp")
b = pd.get_dummies(df['thal'], prefix = "thal")
c = pd.get_dummies(df['slope'], prefix = "slope")

frames = [df, a, b, c]
df = pd.concat(frames, axis = 1)
df.head()

df = df.drop(columns = ['cp', 'thal', 'slope'])
df.head()

y = df.target.values
x_data = df.drop(['target'], axis = 1)

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)

#transpose matrices
x_train = x_train.T
y_train = y_train.T
x_test = x_test.T
y_test = y_test.T

#initialize
def initialize(dimension):
weight = np.full((dimension,1),0.01)
bias = 0.0
return weight,bias

def sigmoid(z):
y_head = 1/(1+ np.exp(-z))
return y_head

def forwardBackward(weight,bias,x_train,y_train):
# Forward
y_head = sigmoid(np.dot(weight.T,x_train) + bias)
loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))
cost = np.sum(loss) / x_train.shape[1]
# Backward
derivative_weight = np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]
derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]
gradients = {"Derivative Weight" : derivative_weight, "Derivative Bias" : derivative_bias}
return cost,gradients

def update(weight,bias,x_train,y_train,learningRate,iteration) :
costList = []
index = []
#for each iteration, update weight and bias values
for i in range(iteration):
cost,gradients = forwardBackward(weight,bias,x_train,y_train)
weight = weight - learningRate * gradients["Derivative Weight"]
bias = bias - learningRate * gradients["Derivative Bias"]
costList.append(cost)
index.append(i)
parameters = {"weight": weight,"bias": bias}
print("iteration:",iteration)
print("cost:",cost)
plt.plot(index,costList)
plt.xlabel("Number of Iteration")
plt.ylabel("Cost")
plt.show()
return parameters, gradients
def predict(weight,bias,x_test):
z = np.dot(weight.T,x_test) + bias
y_head = sigmoid(z)
y_prediction = np.zeros((1,x_test.shape[1]))
for i in range(y_head.shape[1]):
if y_head[0,i] <= 0.5:
y_prediction[0,i] = 0
else:
y_prediction[0,i] = 1
return y_prediction
def logistic_regression(x_train,y_train,x_test,y_test,learningRate,iteration):
dimension = x_train.shape[0]
weight,bias = initialize(dimension)
parameters, gradients = update(weight,bias,x_train,y_train,learningRate,iteration)
y_prediction = predict(parameters["weight"],parameters["bias"],x_test)
print("Manuel Test Accuracy: {:.2f}%".format((100 - np.mean(np.abs(y_prediction - y_test))*100)))
logistic_regression(x_train,y_train,x_test,y_test,1,100)

accuracies = {}
lr = LogisticRegression()
lr.fit(x_train.T,y_train.T)
acc = lr.score(x_test.T,y_test.T)*100
accuracies['Logistic Regression'] = acc
print("Test Accuracy {:.2f}%".format(acc))

# KNN Model
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 2) # n_neighbors means k
knn.fit(x_train.T, y_train.T)
prediction = knn.predict(x_test.T)
print("{} NN Score: {:.2f}%".format(2, knn.score(x_test.T, y_test.T)*100))
# try ro find best k value
scoreList = []
for i in range(1,20):
knn2 = KNeighborsClassifier(n_neighbors = i) # n_neighbors means k
knn2.fit(x_train.T, y_train.T)
scoreList.append(knn2.score(x_test.T, y_test.T))
plt.plot(range(1,20), scoreList)
plt.xticks(np.arange(1,20,1))
plt.xlabel("K value")
plt.ylabel("Score")
plt.show()
acc = max(scoreList)*100
accuracies['KNN'] = acc
print("Maximum KNN Score is {:.2f}%".format(acc))

from sklearn.svm import SVC
svm = SVC(random_state = 1)
svm.fit(x_train.T, y_train.T)
acc = svm.score(x_test.T,y_test.T)*100
accuracies['SVM'] = acc
print("Test Accuracy of SVM Algorithm: {:.2f}%".format(acc))





